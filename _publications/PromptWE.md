---
title: "PromptWE: A fact-checking method based on prompt learning with explanations"
collection: publications
category: conference
permalink: /publication/PromptWE
excerpt: 'In the contemporary "We Media" era, the simplification of news production and dissemination has elevated every individual to the status of news producer and disseminator, and a large amount of false information also follows. Despite the increasing and abundant information on the Internet, the regulation of false information is relatively weak. Consequently, fact-checking is becoming more and more important work, while traditional related work tends to simply label predictions without explaining the reason for the label. The generated explanation in a few studies is also relatively primitive which is hard to comprehend. Because Fact-checking demands a substantial amount of common sense, reasoning, and background knowledge about claims. Prompt learning may further utilize common sense and reasoning ability in pre-trained language models. It may also incorporate the relevant information or additional details within the explanation for claims. In all, it is essential to generate high-quality smooth explanations and further leverage generated explanations for improving classification performance through prompt learning. To address this multifaceted challenge, we propose the PromptWE model (Prompt With Evidence) that uses the prompt learning paradigm to integrate auto-generated explanations with claims. We not only provide natural language explanations that enhance the explainability of the classification result but also further improve the model performance by combining explanation into prompt learning. The model performs hierarchical evidence distillation on many related new reports for every claim to obtain relevant evidence, then uses the BART-CNN model to summarize these incoherent pieces of evidence into one smooth explanation. Consequently, it integrates the claim and explanation into six self-designed templates for prompt learning. Finally, we ensemble the result from different templates to predict the authenticity of the news. Moreover, we replace the generated explanation with the professional explanation from the dataset to investigate the impact of expert evidence on the prompt learning models. Our method achieves good results on two fact-checking datasets: Liar-RAW and RAWFC. Its F1 score is 5% higher than the state-of-the-art model on both datasets at least. We also find that ensemble learning with multiple templates can effectively improve the F1 score of the model. For explanation generation, the model has a higher ROUGE-2 score than the former model. After integrating professional evidence into the prompt templates, the model achieves significant improvement in the classification results on the two datasets, with a maximum improvement of 15% when compared to the results of the PromptWE model. Also, we find that for multi-class classification task, the model with integrated professional evidence exhibited exhibits significant performance improvement on more challenging categories, such as half-true and mostly true. Related experiments indicate that incorporating extracted explanations as supplementary background knowledge about claims, along with the common sense and reasoning abilities learned from pre-trained models, into prompt learning templates can further enhance classification performance for claim veracity. Moreover, sequentially employing the methods of hierarchical evidence extraction and text summarization makes explanations more concise, coherent, and comprehensible. Also, the explanation extracted from unrelated evidence is better suited for integration into prompt learning methods. The further improvement in classification performance after incorporating professional evidence underscores that this approach could swiftly identify accurate and informative prompt templates, facilitating subsequent more efficient utilization of general large models like ChatGPT.'
date: 2024-05-01
venue: 'Volume 64 Issue 5'
slidesurl: 'https://www.sciopen.com/article/10.16511/j.cnki.qhdxxb.2023.27.004'
paperurl: 'https://www.sciopen.com/article/10.16511/j.cnki.qhdxxb.2023.27.004'
---

**任务与贡献**：提出的PromptWE模型在生成任务中通过证据筛选和摘要生成更易理解的解释,然后在分类任务中将解释融合进提示学习模型的提示模板中训练预预训练语言模型之中。不仅可以给出分类，而且给出了相关解释。
**效果**：将生成任务的解释与预训练模型储备的知识相结合，以提高真实性判别的准确率，在 F1 准确率上提升了5%。论文发表在 SMP 会议上，收录于清华大学学报（自然科学版）。




